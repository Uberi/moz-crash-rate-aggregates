{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto\n",
    "conn = boto.connect_s3(host=\"s3-us-west-2.amazonaws.com\")\n",
    "bucket = conn.get_bucket(\"telemetry-spark-emr-2\")\n",
    "db_pass = json.loads(bucket.get_key(\"crash_rate_aggregates_credentials\").get_contents_as_string())[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_host = \"crash-rate-aggregates.cppmil15uwvg.us-west-2.rds.amazonaws.com\"\n",
    "db_name = \"crash_rate_aggregates\"\n",
    "db_user = \"root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "yesterday_utc = (datetime.utcnow() - timedelta(days=1)).strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_date_range = (yesterday_utc, yesterday_utc)\n",
    "spark_context = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "import dateutil.parser\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import numpy as np\n",
    "\n",
    "from moztelemetry.spark import get_pings, get_pings_properties\n",
    "\n",
    "FRACTION = 1.0\n",
    "\n",
    "# paths/dimensions within the ping to compare by, in the same format as the second parameter to `get_pings_properties`\n",
    "# in https://github.com/mozilla/python_moztelemetry/blob/master/moztelemetry/spark.py\n",
    "COMPARABLE_DIMENSIONS = [\n",
    "    \"environment/build/version\",\n",
    "    \"environment/build/buildId\",\n",
    "    \"application/channel\",\n",
    "    \"application/name\",\n",
    "    \"environment/system/os/name\",\n",
    "    \"environment/system/os/version\",\n",
    "    \"environment/build/architecture\",\n",
    "    \"meta/geoCountry\",\n",
    "    \"environment/addons/activeExperiment/id\",\n",
    "    \"environment/addons/activeExperiment/branch\",\n",
    "    \"environment/settings/e10sEnabled\",\n",
    "]\n",
    "\n",
    "# names of the comparable dimensions above, used as dimension names in the database\n",
    "DIMENSION_NAMES = [\n",
    "    \"build_version\",\n",
    "    \"build_date\",\n",
    "    \"channel\",\n",
    "    \"application\",\n",
    "    \"os_name\",\n",
    "    \"os_version\",\n",
    "    \"architecture\",\n",
    "    \"country\",\n",
    "    \"experiment_id\",\n",
    "    \"experiment_branch\",\n",
    "    \"e10s_enabled\",\n",
    "]\n",
    "assert len(COMPARABLE_DIMENSIONS) == len(DIMENSION_NAMES)\n",
    "\n",
    "INSERT_CHUNK_SIZE = 500 # number of records to accumulate in a single database request; higher values mean faster database insertion at the expense of memory usage\n",
    "\n",
    "def compare_crashes(pings, start_date, end_date, comparable_dimensions, dimension_names):\n",
    "    \"\"\"Returns a PairRDD where keys are user configurations and values are Numpy arrays of the form [usage hours, main process crashes, content process crashes, plugin crashes]\"\"\"\n",
    "    ping_properties = get_pings_properties(pings, comparable_dimensions + [\n",
    "        \"meta/submissionDate\",\n",
    "        \"creationDate\",\n",
    "        \"payload/info/subsessionLength\",\n",
    "        \"meta/docType\",\n",
    "        \"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/content\",\n",
    "        \"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/plugin\",\n",
    "        \"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/gmplugin\",\n",
    "    ], with_processes=True)\n",
    "    def is_valid(ping): # sanity check to make sure the ping is actually usable for our purposes\n",
    "        submission_date = ping[\"meta/submissionDate\"]\n",
    "        if not isinstance(submission_date, str) and not isinstance(submission_date, unicode):\n",
    "            return False\n",
    "        activity_date = ping[\"creationDate\"]\n",
    "        if not isinstance(activity_date, str) and not isinstance(activity_date, unicode):\n",
    "            return False\n",
    "        subsession_length = ping[\"payload/info/subsessionLength\"]\n",
    "        if not isinstance(subsession_length, int) and not isinstance(subsession_length, long):\n",
    "            if subsession_length <= 0: # don't allow pings that have invalid subsession lengths\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_crash_pair(ping): # responsible for normalizing a single ping into a crash pair\n",
    "        # we need to parse and normalize the dates here rather than at the aggregates level,\n",
    "        # because we need to normalize and get rid of the time portion\n",
    "\n",
    "        # date the ping was received\n",
    "        submission_date = datetime.strptime(ping[\"meta/submissionDate\"], \"%Y%m%d\").date() # convert the YYYYMMDD format to a real date\n",
    "        submission_date = max(start_date, min(end_date, submission_date)) # normalize the submission date if it's out of range\n",
    "\n",
    "        # date the ping was created on the client\n",
    "        activity_date = dateutil.parser.parse(ping[\"creationDate\"]).date() # the activity date is the date portion of creationDate\n",
    "        activity_date = max(submission_date - timedelta(days=7), min(submission_date, activity_date)) # normalize the activity date if it's out of range\n",
    "\n",
    "        return (\n",
    "            # the keys we want to filter based on\n",
    "            (submission_date, activity_date) + tuple(ping[key] for key in comparable_dimensions), # all the dimensions we can compare by\n",
    "            # the crash values\n",
    "            np.array([\n",
    "                min(25, (ping[\"payload/info/subsessionLength\"] or 0) / 3600.0), # usage hours, limited to ~25 hours to keep things normalized\n",
    "                int(ping[\"meta/docType\"] == \"crash\"), # main crash (is a crash ping)\n",
    "                ping[\"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/content_parent\"] or 0, # content process crashes\n",
    "                (ping[\"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/plugin_parent\"] or 0) +\n",
    "                (ping[\"payload/keyedHistograms/SUBPROCESS_ABNORMAL_ABORT/gmplugin_parent\"] or 0) # plugin crashes\n",
    "            ])\n",
    "        )\n",
    "    crash_values = ping_properties.filter(is_valid).map(get_crash_pair).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    def dimension_mapping(pair): # responsible for converting aggregate crash pairs into individual dimension fields\n",
    "        dimension_key = pair[0]\n",
    "        (submission_date, activity_date), dimension_values = dimension_key[:2], dimension_key[2:]\n",
    "        usage_hours, main_crashes, content_crashes, plugin_crashes = pair[1]\n",
    "        return (\n",
    "            submission_date, activity_date,\n",
    "            {\n",
    "                key: dimension_value\n",
    "                for key, dimension_value in zip(dimension_names, dimension_values)\n",
    "            },\n",
    "            {\n",
    "                \"usage_hours\": usage_hours,\n",
    "                \"main_crashes\": main_crashes,\n",
    "                \"content_crashes\": content_crashes,\n",
    "                \"plugin_crashes\": plugin_crashes,\n",
    "            },\n",
    "        )\n",
    "    return crash_values.map(dimension_mapping)\n",
    "\n",
    "def retrieve_crash_data(sc, submission_date_range, comparable_dimensions, fraction = 0.1):\n",
    "    # get the raw data\n",
    "    normal_pings = get_pings(\n",
    "        sc, doc_type=\"main\",\n",
    "        submission_date=submission_date_range,\n",
    "        fraction=fraction\n",
    "    )\n",
    "    crash_pings = get_pings(\n",
    "        sc, doc_type=\"crash\",\n",
    "        submission_date=submission_date_range,\n",
    "        fraction=fraction\n",
    "    )\n",
    "\n",
    "    return normal_pings.union(crash_pings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date = datetime.strptime(submission_date_range[0], \"%Y%m%d\").date()\n",
    "end_date = datetime.strptime(submission_date_range[1], \"%Y%m%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving pings for ('20160323', '20160323')...\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving pings for {}...\".format(submission_date_range))\n",
    "pings = retrieve_crash_data(spark_context, submission_date_range, COMPARABLE_DIMENSIONS, FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing crashes along dimensions ['build_version', 'build_date', 'channel', 'application', 'os_name', 'os_version', 'architecture', 'country', 'experiment_id', 'experiment_branch', 'e10s_enabled']...\n"
     ]
    }
   ],
   "source": [
    "# compare crashes by all of the above dimensions\n",
    "print(\"Comparing crashes along dimensions {}...\".format(DIMENSION_NAMES))\n",
    "result = compare_crashes(pings, start_date, end_date, COMPARABLE_DIMENSIONS, DIMENSION_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database...\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_pass)\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Setting up database...\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS aggregates (\n",
    "    id serial PRIMARY KEY,\n",
    "    submission_date date NOT NULL,\n",
    "    activity_date date NOT NULL,\n",
    "    dimensions jsonb,\n",
    "    stats jsonb\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# create child tables that inherit from `aggregates`; these partition the data by month for faster querying\n",
    "# when running queries, we can still select from `aggregates`; the query will use the child tables as needed\n",
    "current_month = date(start_date.year, start_date.month, 1)\n",
    "while current_month <= end_date: # loop through the month range\n",
    "    next_month = date(current_month.year, (current_month.month % 12) + 1, 1)\n",
    "    cur.execute(\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}') THEN\n",
    "        CREATE TABLE {table_name} (\n",
    "            CONSTRAINT {table_name}_pk PRIMARY KEY (id),\n",
    "            CONSTRAINT {table_name}_ck CHECK (submission_date >= DATE '{current_month}' AND submission_date < DATE '{next_month}')\n",
    "        ) INHERITS (aggregates);\n",
    "        CREATE INDEX {table_name}_date_idx ON {table_name} (submission_date);\n",
    "        CREATE INDEX {table_name}_dimension_idx ON {table_name} USING gin (dimensions);\n",
    "    END IF;\n",
    "    END\n",
    "    $$\n",
    "    \"\"\".format(\n",
    "        table_name=\"aggregates_partition_{}_{}\".format(current_month.year, current_month.month),\n",
    "        current_month=datetime.strftime(current_month, \"%Y-%m-%d\"),\n",
    "        next_month=datetime.strftime(next_month, \"%Y-%m-%d\"),\n",
    "    ))\n",
    "    current_month = next_month\n",
    "\n",
    "# when we attempt to insert or delete into `aggregates`, redirect it to the proper child table instead\n",
    "# the child table will only store a month's worth of data (~3000000 rows), so queries that only need\n",
    "# a specific submission date range will have much less to look through\n",
    "cur.execute(\"\"\"\n",
    "CREATE OR REPLACE FUNCTION aggregates_insert_trigger()\n",
    "RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "    EXECUTE format('INSERT INTO %I SELECT $1.*', 'aggregates_partition_' || EXTRACT(YEAR FROM NEW.submission_date) || '_' || EXTRACT(MONTH FROM NEW.submission_date)) USING NEW;\n",
    "    RETURN NULL;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "DROP TRIGGER IF EXISTS insert_aggregates_trigger ON aggregates;\n",
    "CREATE TRIGGER insert_aggregates_trigger BEFORE INSERT ON aggregates FOR EACH ROW EXECUTE PROCEDURE aggregates_insert_trigger();\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 existing aggregates for the submission date range 2016-03-23 to 2016-03-23\n"
     ]
    }
   ],
   "source": [
    "# remove previous data for the selected days, if available\n",
    "# this is necessary to be able to backfill data properly\n",
    "cur.execute(\"\"\"DELETE FROM aggregates WHERE submission_date >= %s and submission_date <= %s\"\"\", (start_date, end_date))\n",
    "print(\"Removed {} existing aggregates for the submission date range {} to {}\".format(cur.rowcount, start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting and updating aggregates...\n",
      "Collecting and updating aggregates... 10000 processed\n",
      "Collecting and updating aggregates... 20000 processed\n",
      "Collecting and updating aggregates... 30000 processed\n",
      "Collecting and updating aggregates... 40000 processed\n",
      "Collecting and updating aggregates... 50000 processed\n",
      "Collecting and updating aggregates... 60000 processed\n",
      "Collecting and updating aggregates... 70000 processed\n",
      "Collecting and updating aggregates... 80000 processed\n",
      "Collecting and updating aggregates... 90000 processed\n",
      "Collecting and updating aggregates... 100000 processed\n",
      "Collecting and updating aggregates... 110000 processed\n",
      "Collecting and updating aggregates... 120000 processed\n",
      "Collecting and updating aggregates... 130000 processed\n",
      "Collecting and updating aggregates... 140000 processed\n",
      "Collecting and updating aggregates... 150000 processed\n",
      "Collecting and updating aggregates... 160000 processed\n",
      "Collecting and updating aggregates... 170000 processed\n",
      "Collecting and updating aggregates... 180000 processed\n",
      "Collecting and updating aggregates... 190000 processed\n",
      "Collecting and updating aggregates... 200000 processed\n",
      "Collecting and updating aggregates... 210000 processed\n",
      "Collecting and updating aggregates... 220000 processed\n",
      "Collecting and updating aggregates... 230000 processed\n",
      "Collecting and updating aggregates... 240000 processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting and updating aggregates...\")\n",
    "\n",
    "row_accumulator = [] # do inserts in large chunks for a significantly faster insertion operation while allowing for larger-than-RAM datasets\n",
    "aggregate_count = 0\n",
    "for submission_date, activity_date, dimensions, crash_data in result.toLocalIterator():\n",
    "    aggregate_count += 1 # doing this is actually faster than using result.count()\n",
    "    row_accumulator.append(cur.mogrify(\"(%s, %s, %s, %s)\", (\n",
    "        submission_date,\n",
    "        activity_date,\n",
    "        extras.Json(dimensions),\n",
    "        extras.Json(crash_data),\n",
    "    )))\n",
    "    if len(row_accumulator) >= INSERT_CHUNK_SIZE: # full chunk obtained, perform insert\n",
    "        cur.execute(\"\"\"INSERT INTO aggregates(submission_date, activity_date, dimensions, stats) VALUES {}\"\"\".format(\",\".join(row_accumulator)))\n",
    "        row_accumulator = []\n",
    "    if aggregate_count % 10000 == 0:\n",
    "        print(\"Collecting and updating aggregates... {} processed\".format(aggregate_count))\n",
    "\n",
    "if row_accumulator: # handle any remaining rows that need to be inserted\n",
    "    cur.execute(\"\"\"INSERT INTO aggregates(submission_date, activity_date, dimensions, stats) VALUES {}\"\"\".format(\",\".join(row_accumulator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "JOB COMPLETED SUCCESSFULLY\n",
      "inserted 249634 aggregates\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"JOB COMPLETED SUCCESSFULLY\")\n",
    "print(\"inserted {} aggregates\".format(aggregate_count))\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
