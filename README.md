Crash Rate Aggregates
=====================

Scheduled batch job for computing aggregate crash rates across a variety of criteria.

Development
-----------

First, make sure you have your SPARK_HOME environment variable set to the Apache Spark program directory. For example, `export SPARK_HOME=/home/anthony/Desktop/spark-1.6.0-bin-hadoop2.6`.

To run the tests, execute `python -m unittest discover` in the project directory.

Deployment
----------

Ansible-Playbook is used to deploy the service. Run `ansible-playbook`


There are two parts to the service

* A Telemetry batch job is set up using the [Schedule an analysis job](https://analysis.telemetry.mozilla.org/schedule) functionality of the Telemetry analysis service.
  * The job runs daily under :azhang's account.
  * The job name is `crash-rate-aggregates`, and the tarball is generated by Ansible.
  * Batch jobs run under the Telemetry analysis environment, which includes all the necessary Spark libraries and configuration.