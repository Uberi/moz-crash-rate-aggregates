Crash Rate Aggregates
=====================

Scheduled batch job for computing aggregate crash rates across a variety of criteria.

* A Telemetry batch job runs daily using a [scheduled analysis job](https://analysis.telemetry.mozilla.org/schedule).
    * The job, `crash-rate-aggregates`, runs the project from the `crash-rate-aggregates-job.tar.gz` tarball generated by Ansible.
    * The job takes its needed credentials from the telemetry-spark-emr-2 S3 bucket.
    * Currently, this job is running under :azhang's account every day at noon UTC, with the default settings for everything else. The job is named `crash-rate-aggregates`.
* A large Amazon RDB instance with Postgresql is filled by the analysis job.
    * Currently, this instance is available at `crash-rate-aggregates.cppmil15uwvg.us-west-2.rds.amazonaws.com:5432`, and credentials are available on S3.
* The database is meant to be consumed by [re:dash](https://sql.telemetry.mozilla.org/dashboard/general) to make crash rate dashboards.
    * On the re:dash interface, these aggregates can be used by queries when the query data source is set to "Crash-DB".
    * The database has one table, `crash_aggregates`, that has all of the dimensions, submission dates, and etc.
    * For example, you can get the main process crashes per hour on Nightly for March 14, 2016 with `SELECT sum(main_crashes) / sum(usage_hours) FROM crash_aggregates WHERE dimensions->>'channel' = 'nightly' AND submission_date = '2016-03-14'`.

Development
-----------

First, make sure you have your SPARK\_HOME environment variable set to the Apache Spark program directory. For example, `export SPARK_HOME=/home/anthony/Desktop/spark-1.6.0-bin-hadoop2.6`.

To backfill data, just run `crash_rate_aggregates/fill_database.py` with the desired start/end dates as the `--min-submission-date`/`--max-submission-date` arguments. The operation is idempotent and existing aggregates for those dates are overwritten.

To run the tests, execute `python -m unittest discover` in the project directory.

To add a new dimension to compare on, add matching entries to `COMPARABLE_DIMENSIONS` and `DIMENSION_NAMES` in `crash_rate_aggregates/fill_database.py`.

Deployment
----------

1. Install Ansible Playbook and Boto: `sudo apt-get install software-properties-common python-boto; sudo apt-add-repository ppa:ansible/ansible; sudo apt-get update; sudo apt-get install ansible`.
2. Run `ansible-playbook ansible/deploy.yml` to set up the RDB instance.
3. Set up the scheduled task on the [Telemetry analysis service](https://analysis.telemetry.mozilla.org/schedule) to run daily, using the `crash-rate-aggregates-job.tar.gz` tarball generated by Ansible in step 2.
