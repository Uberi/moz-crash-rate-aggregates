Crash Rate Aggregates
=====================

Scheduled batch job for computing aggregate crash rates across a variety of criteria.

* A Telemetry batch job runs daily using a [scheduled analysis job](https://analysis.telemetry.mozilla.org/schedule).
    * The job, `crash-rate-aggregates`, runs the project from the `crash-rate-aggregates-job.tar.gz` tarball generated by Ansible.
    * The job takes its needed credentials from the telemetry-spark-emr-2 S3 bucket.
    * Currently, this job is running under :azhang's account every day at noon UTC, with the default settings for everything else. The job is named `crash-rate-aggregates`.
* A large Amazon RDB instance with Postgresql is filled by the analysis job.
    * Currently, this instance is available at `crash-rate-aggregates.cppmil15uwvg.us-west-2.rds.amazonaws.com:5432`, and credentials are available on S3.
* The database is meant to be consumed by [re:dash](https://sql.telemetry.mozilla.org/dashboard/general) to make crash rate dashboards.
    * On the re:dash interface, these aggregates can be used by queries when the query data source is set to "Crash-DB".
    * The database has one table, `crash_aggregates`, that has all of the dimensions, submission dates, and etc.
    * For example, you can get the main process crashes per hour on Nightly for March 14, 2016 with `SELECT sum(main_crashes) / sum(usage_hours) FROM crash_aggregates WHERE dimensions->>'channel' = 'nightly' AND submission_date = '2016-03-14'`.

Development
-----------

This project uses [Vagrant](https://www.vagrantup.com/) for setting up the development environment, and [Ansible](https://www.ansible.com/) for automation. You can install these pretty easily with `sudo apt-get install vagrant ansible` on Debian-derivatives.

To set up a development environment, simply run `vagrant up` and then `vagrant ssh` in the project root folder. This will open a terminal within the Vagrant VM. Do `cd /vagrant` to get to the project folder within the VM.

Note that within the Vagrant VM, you should use `~/miniconda2/bin/python` as the main Python binary. All the packages are installed for Miniconda's Python rather than the system Python.

To backfill data, just run `crash_rate_aggregates/fill_database.py` with the desired start/end dates as the `--min-submission-date`/`--max-submission-date` arguments. The operation is idempotent and existing aggregates for those dates are overwritten.

To run the tests, execute `python -m unittest discover` in the `test/` directory.

To add a new dimension to compare on, add matching entries to `COMPARABLE_DIMENSIONS` and `DIMENSION_NAMES` in `crash_rate_aggregates/fill_database.py`.

When running in the Telemetry Analysis Environment, debugging can be a pain. These commands can be helpful:

* `yum install w3m; w3m http://localhost:4040` to show the Spark UI in the command line.
* `tail -f /mnt/spark.log` to view the Spark logs.
* `yum install nethogs; sudo nethogs` to monitor network utilization for each process.

Deployment
----------

1. Install [Ansible Playbook](http://docs.ansible.com/ansible/playbooks.html) and Boto: `sudo apt-get install software-properties-common python-boto; sudo apt-add-repository ppa:ansible/ansible; sudo apt-get update; sudo apt-get install ansible`.
2. Run `ansible-playbook ansible/deploy.yml` to set up the RDB instance on AWS.
3. Set up the scheduled task on the [Telemetry analysis service](https://analysis.telemetry.mozilla.org/schedule) to run daily, using the `crash-rate-aggregates-job.tar.gz` tarball generated by Ansible in step 2.
